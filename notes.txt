CHAPTER - 1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1

> The Wage data involves predicting a continuous or quantitative output value.
This is often referred to as a regression problem

>categorical or qualitative output

>The goal is to predict whether the index
will increase or decrease on a given day using the past 5 days’ percentage
changes in the index. Here the statistical learning problem does not involve
predicting a numerical value. Instead it involves predicting whether
a given day’s stock market performance will fall into the Up bucket or the
Down bucket. This is known as a classification problem.

>Interestingly,
there are hints of some weak trends in the data that suggest that, at least
for this 5-year period, it is possible to correctly predict the direction of
movement in the market approximately 60% of the time

> important class of problems involves
situations in which we only observe input variables, with no corresponding
output: clustering problem

>These
are the first two principal components of the data, which summarize the
6, 830 expression measurements for each cell line down to two numbers or
dimensions

>Deciding on the number of clusters is often a diffi-
cult problem.

>Legendre and Gauss published papers on the method of least squares, which implemented the earliest form of what is now known
as linear regression.

> Linear regression is used for predicting quantitative values,
such as an individual’s salary

> predict qualitative values  : linear discriminant analysis/logistic regression

>generalized
linear models

>they were almost exclusively linear methods, because
fitting non-linear relationships was computationally infeasible at the
time

>non-linear methods : classification and
regression trees, and were among the first to demonstrate the power of a
detailed practical implementation of a method, including cross-validation
for model selection.

>generalized additive
models

>statistical learning has emerged as a new subfield in statistics,
focused on supervised and unsupervised modeling and prediction

>increasing availability of enormous
quantities of data and the software to analyze it.

>n to represent the number of distinct data points, or observations,
in our sample

>p denote the number of variables that are
available for use in making predictions; indicate variable names using colored font

>In some examples, p might be quite large, such as on the order of thousands
or even millions; this situation arises quite often, for example, in the
analysis of modern biological data or web-based advertising data.

>Vectors are by default represented as columns

>GO TO PAPER NOTES

>K-nearest neighbor classifier

>classical linear methods for regression and classification

>linear regression

>important classical classification methods, logistic regression and linear
discriminant analysis

>A central problem in all statistical learning situations involves choosing
the best method for a given application.
cross-validation and the bootstrap, which can be used to estimate the
accuracy of a number of different methods in order to choose the best one

>linear methods often have advantages over
their non-linear competitors in terms of interpretability and sometimes also
accuracy
include stepwise selection, ridge regression,
principal components regression, partial least squares, and the lasso

> non-linear methods
that work well for problems with a single input variable.

> show how
these methods can be used to fit non-linear additive models for which there
is more than one input

>tree-based methods,
including bagging, boosting, and random forests

>Support vector machines,
a set of approaches for performing both linear and non-linear classification

>we consider a setting
in which we have input variables but no output variable. In particular, we
present princip

> DATASETS : The ISLR package
available on the book website contains a number of data sets that are
required in order to perform the labs and exercises associated with this
book
MASS library, and yet another
is part of the base R distribution

>A list of data sets needed to perform the labs and exercises


CHAPTER -2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2

> if we determine that
there is an association between advertising and sales, then we can instruct
our client to adjust advertising budgets, thereby indirectly increasing sales.
In other words, our goal is to develop an accurate model that can be used
to predict sales on the basis of the three media budgets.


> advertising budgets are input variables while sales input
is an output variable


> Since Income is a simulated data set, f is
known 

>We note that some of the
30 observations lie above the blue curve and some lie below it; overall, the
errors have approximately mean zero.

>In general, the function f may involve more than one input variable.
>There are two main reasons that we may wish to estimate f: prediction
and inference

>Prediction

>The accuracy of Yˆ as a prediction for Y depends on two quantities,
which we will call the reducible error and the irreducible error

>^f will not be a perfect estimate for f, and this inaccuracy will introduce
some error. This error is reducible because we can potentially improve the
accuracy of ˆf by using the most appropriate statistical learning technique to
estimate f.

>Therefore, variability
associated with epsilon also affects the accuracy of our predictions. This is known
as the irreducible error, because no matter how well we estimate f, we
cannot reduce the error introduced by epsilon .

>Why is the irreducible error larger than zero? 



>The quantity  may contain
unmeasured variables that are useful in predicting Y : since we don’t
measure them, f cannot use them for its prediction. The quantity  may
also contain unmeasurable variation.


>Inference


> We instead want to understand
the relationship between X and Y , or more specifically, to understand how
Y changes as a function of X1,...,Xp. Now ˆf cannot be treated as a black
box, because we need to know its exact form.

>Which predictors are associated with the response? 
>What is the relationship between the response and each predictor?
>Can the relationship between Y and each predictor be adequately summarized
using a linear equation, or is the relationship more complicated?

>Depending on whether our ultimate goal is prediction, inference, or a
combination of the two, different methods for estimating f may be appropriate.
For example, linear models allow for relatively simple and inter- linear model
pretable inference, but may not yield as accurate predictions as some other
approaches. In contrast, some of the highly non-linear approaches that we
discuss in the later chapters of this book can potentially provide quite accurate
predictions for Y , but this comes at the expense of a less interpretable
model for which inference is more challenging.

>n = 30 data points.
These observations are called the training data because we will use these training data
observations to train, or teach, our method how to estimate f

>Our goal is to apply a statistical learning method to the training data
in order to estimate the unknown function f.

>Broadly
speaking, most statistical learning methods for this task can be characterized
as either parametric or non-parametric. 


>Parametric Methods
		Parametric methods involve a two-step model-based approach.

		make an assumption about the functional form, or shape,
of f

		. After a model has been selected, we need a procedure that uses the
training data to fit or train the model

The most common approach to fitting the model (2.4) is referred to
as (ordinary) least squares

>The model-based approach just described is referred to as parametric;
it reduces the problem of estimating f down to one of estimating a set of parameters. Assuming a parametric form for f simplifies the problem of
estimating f because it is generally much easier to estimate a set of parameters,
such as β0, β1,...,βp in the linear model (2.4), than it is to fit
an entirely arbitrary function f

>The potential disadvantage of a parametric
approach is that the model we choose will usually not match the true
unknown form of f. 

>fit,train, least squares   
>flexible model
>overfittting, noise

>If the chosen model is too far from the true f, then
our estimate will be poor. We can try to address this problem by choosing
flexible models that can fit many different possible functional forms flexible
for f. But in general, fitting a more flexible model requires estimating a
greater number of parameters. These more complex models can lead to a
phenomenon known as overfitting the data, which essentially means they overfitting
follow the errors, or noise, too closely.

>least squares linear regression

>: the true f has some curvature that is not captured in the linear fit.

>It may be
that with such a small number of observations, this is the best we can do. A reasonable enough model not exactly resembling the actual function


>Non-parametric Methods

Non-parametric methods do not make explicit assumptions about the functional
form of f. Instead they seek an estimate of f that gets as close to the
data points as possible without being too rough or wiggly. Such approaches
can have a major advantage over parametric approaches: by avoiding the
assumption of a particular functional form for f, they have the potential
to accurately fit a wider range of possible shapes for f. Any parametric
approach brings with it the possibility that the functional form used to
estimate f is very different from the true f, in which case the resulting
model will not fit the data well. In contrast, non-parametric approaches
completely avoid this danger, since essentially no assumption about the
form of f is made. But non-parametric approaches do suffer from a major
disadvantage: since they do not reduce the problem of estimating f to a
small number of parameters, a very large number of observations (far more
than is typically needed for a parametric approach) is required in order to
obtain an accurate estimate for f.


> thin-plate spline can be used to estimate f

>In order to fit a thin-plate
spline, the data analyst must select a level of smoothness

>the spline fit shown in Figure 2.6 is far more variable than the
true function f

>an example of overfitting the
data, which we discussed previously. It is an undesirable situation because
the fit obtained will not yield accurate estimates of the response on new
observations that were not part of the original training data set\

>methods for choosing the correct amount of smoothness

> The Trade-Off Between Prediction Accuracy and Model
Interpretability

Of the many methods that we examine in this book, some are less flexible,
or more restrictive, in the sense that they can produce just a relatively
small range of shapes to estimate f. For example, linear regression is a
relatively inflexible approach, because it can only generate linear functions

>In general, as the flexibility
of a method increases, its interpretability decreases

>Graph between intepretibility and infrence.

>Other methods, such as the thin plate splines are considerably more flexible because they can generate a much wider
range of possible shapes to estimate f

>why would we ever
choose to use a more restrictive method instead of a very flexible approach?

If we are mainly interested in inference, then restrictive models are much
more interpretable.

For instance, when inference is the goal, the linear
model may be a good choice since it will be quite easy to understand
the relationship between Y and X1, X2,...,Xp.

In contrast, very flexible
approaches, such as the splines and the boosting methods 
can
lead to such complicated estimates of f that it is difficult to understand
how any individual predictor is associated with the response.


> Least
squares linear regression is relatively inflexible but
is quite interpretable

>The lasso relies upon the lasso linear model  but uses an alternative fitting procedure for estimating
the coefficients β0, β1,...,βp. The new procedure is more restrictive in estimating
the coefficients, and sets a number of them to exactly zero. Hence
in this sense the lasso is a less flexible approach than linear regression.
It is also more interpretable than linear regression, because in the final
model the response variable will only be related to a small subset of the
predictors—namely, those with nonzero coefficient estimates.

>Generalized additive models (GAMs),instead extend the lin- generalized
additive
model
ear model (2.4) to allow for certain non-linear relationships. Consequently,
GAMs are more flexible than linear regression. They are also somewhat
less interpretable than linear regression, because the relationship between
each predictor and the response is now modeled using a curve

>fully
non-linear methods such as bagging, boosting, and support vector machines bagging
boosting with non-linear kernels are highly flexible
support
vector
machine
approaches that are harder to interpret


>We have established that when inference is the goal, there are clear advantages
to using simple and relatively inflexible statistical learning methods.
In some settings, however, we are only interested in prediction, and
the interpretability of the predictive model is simply not of interest. For
instance, if we seek to develop an algorithm to predict the price of a
stock, our sole requirement for the algorithm is that it predict accurately—
interpretability is not a concern. In this setting, we might expect that it
will be best to use the most flexible model available. Surprisingly, this is
not always the case! We will often obtain more accurate predictions using
a less flexible method. This phenomenon, which may seem counterintuitive
at first glance, has to do with the potential for overfitting in highly flexible
methods.


> Supervised Versus Unsupervised Learning

>For each observation of the
predictor measurement(s) xi, i = 1,...,n there is an associated response
measurement yi. We wish to fit a model that relates the response to the
predictors, with the aim of accurately predicting the response for future
observations (prediction) or better understanding the relationship between
the response and the predictors (inference). Many classical statistical learning
methods such as linear regression and logistic regression (Chapter 4), as logistic well as more modern approaches such as GAM, boosting, and support vec- regression
tor machines, operate in the supervised learning domain

>In contrast, unsupervised learning describes the somewhat more challenging
situation in which for every observation i = 1,...,n, we observe
a vector of measurements xi but no associated response yi. It is not possible
to fit a linear regression model, since there is no response variable
to predict. In this setting, we are in some sense working blind; the situation
is referred to as unsupervised because we lack a response variable
that can supervise our analysis.
We can seek to understand the relationships between the variables
or between the observations

One statistical learning tool that we may use
in this setting is cluster analysis, or clustering.

>If the information about each customer’s spending patterns
were available, then a supervised analysis would be possible

>If not available : we can try to cluster
the customers on the basis of the variables measured, in order to identify
distinct groups of potential customers.
 Identifying such groups can be of
interest because it might be that the groups differ with respect to some
property of interest, such as spending habits.


>, in practice the group memberships
are unknown, and the goal is to determine the group to which each observation
belongs.

>easy task if the groups are well-separated;

a more challenging problem in which there is some overlap between the groups
in such cases a clustering method could not be expected to assign
all of the overlapping points to their correct group

>scatterplots

>in practice, we often encounter data
sets that contain many more than two variables
In this case, we cannot
easily plot the observations. For instance, if there are p variables in our
data set, then p(p − 1)/2 distinct scatterplots can be made, and visual
inspection is simply not a viable way to identify clusters. For this reason,
automated clustering methods are important.

>sometimes the question of whether an analysis
should be considered supervised or unsupervised is less clear-cut

>semi-supervised learning : In this setting, we wish to use a sta- semisupervised
learning
tistical learning method that can incorporate the m observations for which
response measurements are available as well as the n − m observations for
which they are not.

>Regression Versus Classification Problems

Variables can be characterized as either quantitative or qualitative (also quantitative
qualitative known as categorical).

We tend to refer to problems
with a quantitative response as regression problems, while those involv- regression
ing a qualitative response are often referred to as classification problems.

> Least squares linear regression is used with a quantitative response, whereas logistic
regression is typically used with a qualitative (two-class, or
binary) response. As such it is often used as a classification method. But binary
since it estimates class probabilities, it can be thought of as a regression method as well. 

K-nearest neighbors  and boosting can be used in the case of either quantitative or qualitative responses.


>whether the predictors are qualitative or quantitative is generally considered
less important. Most of the statistical learning methods discussed in
this book can be applied regardless of the predictor variable type, provided
that any qualitative predictors are properly coded before the analysis is
performed.


>Assessing Model Accuracy


>There
is no free lunch in statistics: no one method dominates all others over all
possible data sets. On a particular data set, one specific method may work
best, but some other method may work better on a similar but different
data set. Hence it is an important task to decide for any given set of data
which method produces the best results. Selecting the best approach can
be one of the most challenging parts of performing statistical learning in
practice.

>Measuring the Quality of Fit

>we need to quantify the extent
to which the predicted response value for a given observation is close to
the true response value for that observation.

> In the regression setting, the
most commonly-used measure is the mean squared error (MSE),

>The MSE is computed using the training data that was used to
fit the model, and so should more accurately be referred to as the training
MSE. But in general, we do not really care how well the method works training
on the training data.

>Rather, we are interested in the accuracy of the pre- MSE
dictions that we obtain when we apply our method to previously unseen
test data

>But we don’t really care how well our method predicts last week’s
stock price. We instead care about how well it will predict tomorrow’s price
or next month’s price

>we want to know whether ˆf(x0) is approximately equal
to y0, where (x0, y0) is a previously unseen test observation not used to train
the statistical learning method.

>Why  average squared prediction error ? FIND THE ANSER TO THIS QUESTION.

>the average squared prediction error for these test observations (x0, y0).
We’d like to select the model for which the average of this quantity—the
test MSE—is as small as possible



>But what if no test observations are available? In that case, one
might imagine simply selecting a statistical learning method that minimizes
the training MSE (2.5). This seems like it might be a sensible approach,
since the training MSE and the test MSE appear to be closely related.
Unfortunately, there is a fundamental problem with this strategy: there
is no guarantee that the method with the lowest training MSE will also
have the lowest test MSE. Roughly speaking, the problem is that many
statistical methods specifically estimate coefficients so as to minimize the
training set MSE. For these methods, the training set MSE can be quite
small, but the test MSE is often much larger.


>It is smoothing
clear that as the level of flexibility increases, the curves fit the observed spline
data more closely. 


>By adjusting the level of flexibility of the
smoothing spline fit, we can produce many different fits to this data

>the degrees of freedom, for a number of smoothing splines. The de- degrees of
grees of freedom is a quantity that summarizes the flexibility of a curve

>A more restricted and hence smoother curve has fewer degrees
of freedom than a wiggly curve

>The training
MSE declines monotonically as flexibility increases


>In this example, we know the true function f, and so we can also compute
the test MSE over a very large test set, as a function of flexibility. (Of
course, in general f is unknown, so this will not be possible.)


>As
with the training MSE, the test MSE initially declines as the level of flexibility
increases. However, at some point the test MSE levels off and then
starts to increase again (overfitting)

> The horizontal dashed line indicates Var(),
the irreducible error in (2.3), which corresponds to the lowest achievable
test MSE among all possible methods.

>as the flexibility of the statistical
learning method increases, we observe a monotone decrease in the training
MSE and a U-shape in the test MSE. This is a fundamental property of
statistical learning that holds regardless of the particular data set at hand
and regardless of the statistical method being used. As model flexibility
increases, training MSE will decrease, but the test MSE may not. When
a given method yields a small training MSE but a large test MSE, we are
said to be overfitting the data. 

>as the flexibility of the statistical
learning method increases, we observe a monotone decrease in the training
MSE and a U-shape in the test MSE. This is a fundamental property of
statistical learning that holds regardless of the particular data set at hand
and regardless of the statistical method being used. As model flexibility
increases, training MSE will decrease, but the test MSE may not. When
a given method yields a small training MSE but a large test MSE, we are
said to be overfitting the data. 



> the
flexibility level corresponding to the model with the minimal test MSE can
vary considerably among data sets. Throughout this book, we discuss a
variety of approaches that can be used in practice to estimate this minimum
point. One important method is cross-validation (Chapter 5), which is a crossmethod
for estimating test MSE using the training data.

>cross validation


>The Bias-Variance Trade-Off

What do we mean by the variance and bias of a statistical learning
method? Variance refers to the amount by which ˆf would change if we
estimated it using a different training data set. Since the training data
are used to fit the statistical learning method, different training data sets
will result in a different ˆf. But ideally the estimate for f should not vary
too much between training sets. However, if a method has high variance
then small changes in the training data can result in large changes in ˆf. In
>general, more flexible statistical methods have higher variance. 

>On the other hand, bias refers to the error that is introduced by approximating
a real-life problem, which may be extremely complicated, by a much
simpler model. For example, linear regression assumes that there is a linear
relationship between Y and X1, X2,...,Xp. It is unlikely that any real-life
problem truly has such a simple linear relationship, and so performing linear
regression will undoubtedly result in some bias in the estimate of 

>if the true f is substantially non-linear, so no matter how many
training observations we are given, it will not be possible to produce an
accurate estimate using linear regression. In other words, linear regression
results in high bias in this example

>Generally, more flexible
methods result in less bias.
>As a general rule, as we use more flexible methods, the variance will
increase and the bias will decrease.

> The relative rate of change of these
two quantities determines whether the test MSE increases or decreases.

>As
we increase the flexibility of a class of methods, the bias tends to initially
decrease faster than the variance increases. Consequently, the expected
test MSE declines. However, at some point increasing flexibility has little
impact on the bias but starts to significantly increase the variance. When
this happens the test MSE increases.

>However, the flexibility
level corresponding to the optimal test MSE differs considerably among the
three data sets, because the squared bias and variance change at different
rates in each of the data sets. 

>The relationship between bias, variance, and test set MSE given in Equation
2.7 and displayed in Figure 2.12 is referred to as the bias-variance
trade-off
> Good test set performance of a statistical learning method re- bias-variance
quires low variance as well as low squared bias.
This is referred to as a trade-off
trade-off because it is easy to obtain a method with extremely low bias but
high variance (for instance, by drawing a curve that passes through every
single training observation) or a method with very low variance but high
bias (by fitting a horizontal line to the data). The challenge lies in finding
a method for which both the variance and the squared bias are low. This
trade-off is one of the most important recurring themes in this book.


>In a real-life situation in which f is unobserved, it is generally not possible
to explicitly compute the test MSE, bias, or variance for a statistical
learning method. Nevertheless, one should always keep the bias-variance
trade-off in mind. 

However, this does not
guarantee that they will outperform a much simpler method such as linear
regression. To take an extreme example, suppose that the true f is linear.
In this situation linear regression will have no bias, making it very hard
for a more flexible method to compete. In contrast, if the true f is highly
non-linear and we have an ample number of training observations, then
we may do better using a highly flexible approach,

>cross-validation, which is a way to estimate the test
MSE using the training data.



>The Classification Setting : indicator variable, wherether a class mathes its label or not.

>The Bayes Classifier  : It is possible to show (though the proof is outside of the scope of this
book) that the test error rate given in (2.9) is minimized, on average, by a
very simple classifier that assigns each observation to the most likely class,
given its predictor values.

>conditional probability

>Bayes decision boundary

>The Bayes error rate is analogous to
the irreducible error, discussed earlier


>K-Nearest Neighbors

>In theory we would always like to predict qualitative responses using the
Bayes classifier. But for real data, we do not know the conditional distribution
of Y given X, and so computing the Bayes classifier is impossible.


>Therefore, the Bayes classifier serves as an unattainable gold standard
against which to compare other methods.

> Many approaches attempt to
estimate the conditional distribution of Y given X, and then classify a
given observation to the class with highest estimated probability. One such
method is the K-nearest neighbors (KNN) classifier.

>using K = 1 and K = 100. When K = 1, the decision boundary is overly
flexible and finds patterns in the data that don’t correspond to the Bayes
decision boundary. This corresponds to a classifier that has low bias but
very high variance. As K grows, the method becomes less flexible and
produces a decision boundary that is close to linear. This corresponds to
a low-variance but high-bias classifier

>In both the regression and classification settings, choosing the correct
level of flexibility is critical to the success of any statistical learning method.
The bias-variance tradeoff, and the resulting U-shape in the test error, can
make this a difficult task



R Programming language : 

>R uses functions to perform operations. To run a function called funcname, function
we type funcname(input1, input2), where the inputs (or arguments) input1 

>For example, to create a vector of numbers, we use the function
c() (for concatenate). 

>console : Hitting the up arrow multiple times will display the previous commands,
which can then be edited. This is useful since one often wishes to repeat
a similar command.

>matrix : by
default R creates matrices by successively filling in columns

>By default, rnorm() creates standard normal random variables with a mean
of 0 and a standard deviation of 1.


>Sometimes we want our code to reproduce the exact same set of random
numbers; we can use the set.seed() function to do this.

>We use set.seed() throughout the labs whenever we perform calculations
involving random quantities.

>The mean() and var() functions can be used to compute the mean and mean()
var() variance of a vector of numbers. Applying sqrt() to the output of var()
will give the standard deviation.


>It is often a good idea
to view a data set using a text editor or other software such as Excel before
loading it into R.

> c(), length(), ls(), rm(),rm(lsit=ls())

>matrix(), sqrt(), x^2, rnorm(), cor()

>set.seed(), mean(), var(), sd(), sqrt(var())

>plot(),pdf(), image(), dev.off()

>seq(), 1:10, outer(), countour(), t(),image(), persp(), dim()

>read.table(), write.table(), read.csv(), getwd(), setwd()

>fix(), na.omit(),names()

>attach()

>plot(a$1, a$2)

>hist(), pairs(), identify(),summary()

>savehistory(), loadhistory(), q()

