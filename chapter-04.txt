>in many situations, the response
variable is instead qualitative

>Often qualitative variables are referred
to as categorical

>approaches for predicting qualitative responses, a process that
is known as classification.


>Predicting a qualitative response for an obser- classification
vation can be referred to as classifying that observation, since it involves
assigning the observation to a category, or class

> On the other hand, often
the methods used for classification first predict the probability of each of
the categories of a qualitative variable, as the basis for making the classification.
In this sense they also behave like regression methods

>classification techniques, or classifiers, that one might use to predict a qualitative response.

>logistic regression

>linear discriminant analysis

>K-nearest neighbors

>generalized additive models

>trees

>random forests

>boosting

>support vector machines

>Just as in the regression setting, in the classification setting we have a
set of training observations (x1, y1),...,(xn, yn) that we can use to build
a classifier. We want our classifier to perform well not only on the training
data, but also on test observations that were not used to train the classifier

> We are interested in predicting whether an
individual will default on his or her credit card payment, on the basis of
annual income and monthly credit card balance.


>Unfortunately, in general there is no natural way to
convert a qualitative response variable with more than two levels into a
quantitative response that is ready for linear regression

>if we use linear regression, some of our estimates might be
outside the [0, 1] interval , making them hard to interpret
as probabilities!

> it turns out that
the classifications that we get if we use linear regression to predict a binary
response will be the same as for the linear discriminant analysis (LDA)

>However, the dummy variable approach cannot be easily extended to
accommodate qualitative responses with more than two levels. For these
reasons, it is preferable to use a classification method that is truly suited
for qualitative response values, such as the ones presented next.


>Logistic Regression

>Any time a straight line is fit to a binary response that is coded as 0 or 1, in principle we can always predict p(X) < 0 for some values of X
and p(X) > 1 for others (unless the range of X is limited).

>. In logistic regression, we use the logistic function

> logistic log-odds
logit regression model (4.2) has a logit that is linear in X.

>To fit the model, we use a method called maximum likelihood.

>The logistic function will always produce
an S-shaped curve of this form, and so regardless of the value of X, we
will obtain a sensible prediction.


>The average fitted probability in both cases is
0.0333 (averaged over the training data), which is the same as the overall
proportion of defaulters in the data set.


>odds = p(x)/(1-p(x))

>logit = log(odds)

>the logistic log-odds
logit regression model  has a logit that is linear in X

>in a linear regression model, β1 gives the
average change in Y associated with a one-unit increase in X. In contrast,
in a logistic regression model, increasing X by one unit changes the log odds
by β1 (4.4), or equivalently it multiplies the odds by eβ1 (4.3). However,
because the relationship between p(X) and X in (4.2) is not a straight line β1 does not correspond to the change in p(X) associated with a one-unit
increase in X


>maximum likelihood is preferred, since it has better statistical
properties


>The estimated intercept in Table 4.1
is typically not of interest; its main purpose is to adjust the average fitted
probabilities to the proportion of ones in the data.


>Making Predictions

>
As in the linear regression setting, the
results obtained using one predictor may be quite different from those obtained
using multiple predictors, especially when there is correlation among
the predictors. 


>confounding


> Logistic Regression for >2 Response Classes


>discriminant analysis, is popular for multiple-class classification


>Linear Discriminant Analysis


>the
logistic function, given by (4.7) for the case of two response classes. In
statistical jargon, we model the conditional distribution of the response Y ,
given the predictor(s) X. We now consider an alternative and less direct
approach to estimating these probabilities. In this alternative approach,
we model the distribution of the predictors X separately in each of the
response classes (i.e. given Y ), and then use Bayes’ theorem to flip these
around into estimates for Pr(Y = k|X = x)


>When these distributions are
assumed to be normal(distribution), it turns out that the model is very similar in form
to logistic regression.


>To reiterate, the LDA classifier results from assuming that the observations
within each class come from a normal distribution with a class-specific
mean vector and a common variance σ2, and plugging estimates for these
parameters into the Bayes classifier


>confusion matrix

>sensitivity and specificity characterize the performance of
a classifier or screening test.

>Decrease the threshold value if you want better resukts but it may come at a price of false positives. Choice is based on the domain knowledge.

>The ROC curve is a popular graphic for simultaneously displaying the
two types of errors for all possible thresholds.


>ROC : receiver operating characteristics

>area under the (ROC) curve (AUC)

>varying the classifier threshold changes its true
positive and false positive rate. These are also called the sensitivity and one minus the specificity of our classifier.

>a quadratic function in (4.23). This is where QDA gets its name.
Why does it matter whether or not we assume that the K classes share a
common covariance matrix? In other words, why would one prefer LDA to
QDA, or vice-versa? The answer lies in the bias-variance trade-off. When
there are p predictors, then estimating a covariance matrix requires esti-
mating p(p+1)/2 parameters. QDA estimates a separate covariance matrix
for each class, for a total of Kp(p+1)/2 parameters.

By instead assum-ing that the K classes share a common covariance matrix, the LDA model
becomes linear in x, which means there are Kp linear coefficients to esti-
mate.

>Consequently, LDA is a much less flexible classifier than QDA, and
so has substantially lower variance. This can potentially lead to improved
prediction performance. But there is a trade-off: if LDA’s assumption that
the K classes share a common covariance matrix is badly off, then LDA
can suffer from high bias. Roughly speaking, LDA tends to be a better bet
than QDA if there are relatively few training observations and so reducing
variance is crucial. In contrast, QDA is recommended if the training set is
very large, so that the variance of the classifier is not a major concern, or if
the assumption of a common covariance matrix for the K classes is clearly
untenable.

>both logistic re-
gression and LDA produce linear decision boundaries. The only difference
between the two approaches lies in the fact that β 0 and β 1 are estimated
using maximum likelihood, whereas c 0 and c 1 are computed using the esti-
mated mean and variance from a normal distribution. This same connection
between LDA and logistic regression also holds for multidimensional data
with p > 1.

>Since logistic regression and LDA differ only in their fitting procedures,
one might expect the two approaches to give similar results. This is often,
but not always, the case. LDA assumes that the observations are drawn
from a Gaussian distribution with a common covariance matrix in each
class, and so can provide some improvements over logistic regression when
this assumption approximately holds. Conversely, logistic regression can
outperform LDA if these Gaussian assumptions are not met.

>KNN takes a completely different approach
from the classifiers seen in this chapter. In order to make a prediction for
an observation X = x, the K training observations that are closest to x are
identified. Then X is assigned to the class to which the plurality of these
observations belong. Hence KNN is a completely non-parametric approach:
no assumptions are made about the shape of the decision boundary. There-
fore, we can expect this approach to dominate LDA and logistic regression
when the decision boundary is highly non-linear. On the other hand, KNN
does not tell us which predictors are important;

>Finally, QDA serves as a compromise between the non-parametric KNN
method and the linear LDA and logistic regression approaches. Since QDA
assumes a quadratic decision boundary, it can accurately model a wider
range of problems than can the linear methods. Though not as flexible
as KNN, QDA can perform better in the presence of a limited number of
training observations because it does make some assumptions about the
form of the decision boundary.

>KNN and KNN-CV(KNN cross validatory) : Chooosing K classes

>Finally, QDA serves as a compromise between the non-parametric KNN
method and the linear LDA and logistic regression approaches. Since QDA
assumes a quadratic decision boundary, it can accurately model a wider
range of problems than can the linear methods. Though not as flexible
as KNN, QDA can perform better in the presence of a limited number of
training observations because it does make some assumptions about the
form of the decision boundary.

>Finally, recall from Chapter 3 that in the regression setting we can accom-
modate a non-linear relationship between the predictors and the response
by performing regression using transformations of the predictors. A similar
approach could be taken in the classification setting. For instance, we could
create a more flexible version of logistic regression by including X 2 , X 3 ,
and even X 4 as predictors. This may or may not improve logistic regres-
sion’s performance, depending on whether the increase in variance due to
the added flexibility is offset by a sufficiently large reduction in bias. We
could do the same for LDA. If we added all possible quadratic terms and
cross-products to LDA, the form of the model would be the same as the
QDA model, although the parameter estimates would be different. This
device allows us to move somewhere between an LDA and a QDA model.



>library(), names(),dim(), summary(), pairs(), cor()

>attach(), plot(), glm(), coef(), predict(), contrasts()

>rep(), table(), mean(), glm(), lda(), sum() 

>qda(), cbind(), set.seed(), knn(), scale(), var(), sd()
