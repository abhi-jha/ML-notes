>linear regression, a very simple approach for
supervised learning

>predicting
a quantitative response. 

>many fancy statistical learning approaches can be seen as generalizations
or extensions of linear regression. 

>a few important questions that we might
seek to address

>How accurately can we predict?

>Is the relationship linear?
If there is approximately a straight-line relationship between advertising
expenditure in the various media and sales, then linear regression
is an appropriate tool. If not, then it may still be possible to transform
the predictor or the response so that linear regression can be
used.


>a synergy effect, while in statistics it is called an interaction effect

>Simple Linear Regression
It assumes that there is approximately a linear
relationship between X and Y

>before we can make
predictions, we must use data to estimate the coefficients

>In other words, we want to find an intercept βˆ_0 and a slope βˆ_1 such
that the resulting line is as close as possible to the 

>There are a number of ways of measuring closeness

>minimizing the least squares criterion

>the fit makes a compromise
by averaging errors' squares. 

>least squares coefficient estimates for simple linear
regression

> Assessing the Accuracy of the Coefficient Estimates

> β0 is the intercept term—that is, the expected value of Y when X = 0,
and β1 is the slope—the average increase in Y associated with a one-unit
increase in X

>lope—the average increase in Y associated with a one-unit
increase in X. The error term is a catch-all for what we miss with this
simple model: the true relationship is probably not linear, there may be
other variables that cause variation in Y , and there may be measurement
error. We typically assume that the error term is independent of X

>in real applications,
we have access to a set of observations from which we can compute the
least squares line; however, the population regression line is unobserved

>population

>sample

> the unknown coefficients β0 and β1 in linear regression define the
population regression line. We seek to estimate these unknown coefficients
using βˆ_0 and βˆ_1

>For example, suppose that we are interested in knowing
the population mean μ of some random variable Y . Unfortunately, μ is
unknown, but we do have access to n observations from Y

>The sample
mean and the population mean are different, but in general the sample
mean will provide a good estimate of the population mean.


>If we use the bias
sample mean ˆμ to estimate μ, this estimate is unbiased, in the sense that unbiased
on average, we expect ˆμ to equal μ. What exactly does this mean? It means
that on the basis of one particular set of observations y1,...,yn, ˆμ might
overestimate μ, and on the basis of another set of observations, ˆμ might
underestimate μ. But if we could average a huge number of estimates of
μ obtained from a huge number of sets of observations, then this average
would exactly equal μ

>an unbiased estimator does not systematically
over- or under-estimate the true parameter

>The property of unbiasedness
holds for the least squares coefficient estimates

> if
we estimate β0 and β1 on the basis of a particular data set, then our
estimates won’t be exactly equal to β0 and β1. But if we could average
the estimates obtained over a huge number of data sets, then the average
of these estimates would be spot on! In fact, we can see from the righthand
panel of Figure 3.3 that the average of many least squares lines, each
estimated from a separate data set, is pretty close to the true population
regression line.

>We have established that the
average of ˆμ’s over many data sets will be very close to μ, but that a
single estimate ˆμ may be a substantial underestimate or overestimate of μ.
How far off will that single estimate of ˆμ be?

>standard error of ˆμ

>This formula holds provided that the n observations are uncorrelated.

>standard error tells us the average amount that this
estimate ˆμ differs from the actual value of μ

>—the more observations we have, the smaller
the standard error of ˆμ

>Notice in the formula that SE(βˆ1) is
smaller when the xi are more spread out; intuitively we have more leverage
to estimate a slope when this is the case.

>Standard errors can be used to compute confidence intervals.

>Standard errors can also be used to perform hypothesis tests on the hypothesis
coefficients. The most common hypothesis test involves testing the null test
hypothesis 

>Roughly p-value
speaking, we interpret the p-value as follows: a small p-value indicates that
it is unlikely to observe such a substantial association between the predictor
and the response due to chance, in the absence of any real association
between the predictor and the response. Hence, if we see a small p-value,then we can infer that there is an association between the predictor and the
response. We reject the null hypothesis—that is, we declare a relationship
to exist between X and Y —if the p-value is small enough. Typical p-value
cutoffs for rejecting the null hypothesis are 5 or 1 %.

>Assessing the Accuracy of the Model

>The RSE is an estimate of the standard deviation of 

>The RSE is considered a measure of the lack of fit of the model to
the data

>TSS measures the total variance in the response Y , and can be squares
thought of as the amount of variability inherent in the response before the
regression is performed. In contrast, RSS measures the amount of variability
that is left unexplained after performing the regression

>Multiple Linear Regression

>Simple linear regression is a useful approach for predicting a response on the
basis of a single predictor variable

>in practice we often have more
than one predictor

>However, the approach of fitting a separate simple linear regression model
for each predictor is not entirely satisfactory. First of all, it is unclear how to
make a single prediction of sales given levels of the three advertising media
budgets, since each of the budgets is associated with a separate regression
equation. Second, each of the three regression equations ignores the other
two media in forming estimates for the regression coefficients. We will see
shortly that if the media budgets are correlated with each other in the 200
markets that constitute our data set, then this can lead to very misleading
estimates of the individual media effects on sales.

> a better approach is to extend the simple linear regression model so that it can directly accommodate multiple predictors

>We can do
this by giving each predictor a separate slope coefficient in a single model.

> Unlike the simple linear regression
estimates , the multiple regression coefficient estimates have
somewhat complicated forms that are most easily represented using matrix
algebra


> in a simple linear regression which only examines sales
versus newspaper, we will observe that higher values of newspaper tend to be
associated with higher values of sales, even though newspaper advertising
does not actually affect sales. So newspaper sales are a surrogate for radio
advertising; newspaper gets “credit” for the effect of radio on sales.

>Questions for multiple linear regression

1. Is at least one of the predictors X1, X2,...,Xp useful in predicting
the response?
2. Do all the predictors help to explain Y , or is only a subset of the
predictors useful?
3. How well does the model fit the data?
4. Given a set of predictor values, what response value should we predict,
and how accurate is our prediction?

>However, what if the F-statistic had been closer to
1? How large does the F-statistic need to be before we can reject H0 and
conclude that there is a relationship?


>When H0 is true
and the errors i have a normal distribution, the F-statistic follows an
F-distribution.6 For any given value of n and p, any statistical software
package can be used to compute the p-value associated with the F-statistic
using this distribution. Based on this p-value, we can determine whether
or not to reject H0. For the advertising data, the p-value associated with
the F-statistic in Table 3.6 is essentially zero, so we have extremely strong
evidence that at least one of the media is associated with increased sales.


>we are testing H0 that all the coefficients are zero. Sometimes
we want to test that a particular subset of q of the coefficients are zero

>for convenience we have put the variables chosen for omission at the
end of the list

>Notice that in Table 3.4, for each individual predictor a t-statistic and
a p-value were reported. These provide information about whether each
individual predictor is related to the response, after adjusting for the other
predictors. It turns out that each of these are exactly equivalent7 to the
F-test that omits that single variable from the model, leaving all the others
in—i.e. q=1 in (3.24). So it reports the partial effect of adding that variable
to the model. For instance, as we discussed earlier, these p-values indicate
that TV and radio are related to sales, but that there is no evidence that
newspaper is associated with sales, in the presence of these two


>Given these individual p-values for each variable, why do we need to look
at the overall F-statistic? After all, it seems likely that if any one of the
p-values for the individual variables is very small, then at least one of the
predictors is related to the response. However, this logic is flawed, especially
when the number of predictors p is large.
For instance, consider an example in which p = 100 and H0 : β1 = β2 =
... = βp = 0 is true, so no variable is truly associated with the response. In
this situation, about 5 % of the p-values associated with each variable (of
the type shown in Table 3.4) will be below 0.05 by chance. In other words,
we expect to see approximately five small p-values even in the absence of
any true association between the predictors and the response. In fact, we
are almost guaranteed that we will observe at least one p-value below 0.05
by chance! Hence, if we use the individual t-statistics and associated pvalues
in order to decide whether or not there is any association between
the variables and the response, there is a very high chance that we will
incorrectly conclude that there is a relationship. However, the F-statistic
does not suffer from this problem because it adjusts for the number of
predictors. Hence, if H0 is true, there is only a 5 % chance that the Fstatistic
will result in a p-value below 0.05, regardless of the number of
predictors or the number of observations.


>The approach of using an F-statistic to test for any association between
the predictors and the response works when p is relatively small, and certainly
small compared to n. However, sometimes we have a very large number
of variables. If p>n then there are more coefficients βj to estimate
than observations from which to estimate them. In this case we cannot
even fit the multiple linear regression model using least squares

>the first step in a multiple regression
analysis is to compute the F-statistic and to examine the associated pvalue

>If we conclude on the basis of that p-value that at least one of the
predictors is related to the response, then it is natural to wonder which are
the guilty ones!


>We could look at the individual p-values as in Table 3.4,
but as discussed, if p is large we are likely to make some false discoveries.

>It is possible that all of the predictors are associated with the response,
but it is more often the case that the response is only related to a subset of
the predictors. The task of determining which predictors are associated with
the response, in order to fit a single model involving only those predictors,
is referred to as variable selection. 

>Ideally, we would like to perform variable selection by trying out a lot of
different models, each containing a different subset of the predictors. For
instance, if p = 2, then we can consider four models: (1) a model containing
no variables, (2) a model containing X1 only, (3) a model containing
X2 only, and (4) a model containing both X1 and X2. We can then select
the best model out of all of the models that we have considered. How
do we determine which model is best? Various statistics can be used to
judge the quality of a model. These include Mallow’s Cp, Akaike informa- Mallow’s Cp
tion criterion (AIC), Bayesian information criterion (BIC), and adjusted Akaike
information
criterion
Bayesian
information
criterion
R2. These are discussed in more detail in Chapter 6. We can also deteradjusted
R2
mine which model is best by plotting various model outputs, such as the
residuals, in order to search for patterns.


>Therefore, unless p is very
small, we cannot consider all 2p models, and instead we need an automated
and efficient approach to choose a smaller set of models to consider.

>Forward selection, Backward selection, Mixed selection

>Backward selection cannot be used if p>n, while forward selection can
always be used. Forward selection is a greedy approach, and might include
variables early that later become redundant. Mixed selection can remedy
this.

>Model fit

Two of the most common numerical measures of model fit are the RSE and
R^2

>one property of the fitted linear model is
that it maximizes this correlation among all possible linear models.

It turns out that R2 will always increase when more variables are added to the model, even if those variables are only weakly associated
with the response.This is due to the fact that adding another variable to
the least squares equations must allow us to fit the training data (though
not necessarily the testing data) more accurately(overfitting).

>An R^2 value close to 1 indicates that the model explains a large portion
of the variance in the response variable

>models with
more variables can have higher RSE if the decrease in RSS is small relative
to the increase in p

>In addition to looking at the RSE and R2 statistics just discussed, it
can be useful to plot the data

>Graphical summaries can reveal problems
with a model that are not visible from numerical statistics


>a synergy or interaction effect between the advertising
media, whereby combining the media together results in a bigger boost to
sales than using any single medium

> extending
the linear model to accommodate such synergistic effects through
the use of interaction terms

>Predictions

1.The inaccuracy in the coefficient estimates is related to the reducible
error
We can compute a confidence interval in order
to determine how close Yˆ will be to f(X).

2.Of course, in practice assuming a linear model for f(X) is almost
always an approximation of reality, so there is an additional source of
potentially reducible error which we call model bias. So when we use a
linear model, we are in fact estimating the best linear approximation
to the true surface. However, here we will ignore this discrepancy,
and operate as if the linear model were correct.

3.irreducible error : We use prediction intervals to answer this question

Prediction
intervals are always wider than confidence intervals, because they
incorporate both the error in the estimate for f(X) (the reducible
error) and the uncertainty as to how much an individual point will
differ from the population regression plane (the irreducible error)


>We use a confidence interval to quantify the uncertainty surrounding confidence
the average sales over a large number of cities

>Qualitative Predictors : we have assumed that all variables in our linear
regression model are quantitative . often some predictors are qualitative.

>The level with no dummy variable—African
American in this example—is known as the baseline.

>There are many different ways of coding qualitative variables besides
the dummy variable approach taken here

>Extensions of the Linear Model :  Two
of the most important assumptions state that the relationship between the
predictors and response are additive and linear
The additive assumption a means that the effect of changes in a predictor Xj on the response Y is
independent of the values of the other predictors. The linear assumption
states that the change in the response Y due to a one-unit change in Xj is
constant, regardless of the value of Xj . In this book, we examine a number
of sophisticated methods that relax these two assumptions.
>as a synergy effect,
and in statistics it is referred to as an interaction effect

>One way of extending this model
to allow for interaction effects is to include a third predictor, called an
interaction term, which is constructed by computing the product of X1
and X2

>it is
sometimes the case that an interaction term has a very small p-value, but
the associated main effects (in this case, TV and radio) do not. The hierarchical
principle states that if we include an interaction in a model, we hierarchical
should also include the main effects, even if the p-values associated with principle
their coefficients are not significant. In other words, if the interaction between
X1 and X2 seems important, then we should include both X1 and
X2 in the model even if their coefficient estimates have large p-values. The
rationale for this principle is that if X1 × X2 is related to the response,
then whether or not the coefficients of X1 or X2 are exactly zero is of little
interest. Also X1 × X2 is typically correlated with X1 and X2, and so
leaving them out tends to alter the meaning of the interaction.


>the concept of
interactions applies just as well to qualitative variables, or to a combination
of quantitative and qualitative variables.

>Non-linear Relationships

approach that we have just described for extending the linear model
to accommodate non-linear relationships is known as polynomial regression,
since we have included polynomial functions of the predictors in the
regression model.

IT IS STILL LINEAR IN COEFFICIENTS.

>Potential Problems

1. Non-linearity of the response-predictor relationships.
2. Correlation of error terms.
3. Non-constant variance of error terms.
4. Outliers.
5. High-leverage points.
6. Collinearity.

>Residual plots are a useful graphical tool for identifying non-linearity. residual plot
Given a simple linear regression model, we can plot the residuals, ei =
yi − yˆi, versus the predictor xi.


>In the case of a multiple regression model,since there are multiple predictors, we instead plot the residuals versus
the predicted (or fitted) values ˆyi. Ideally, the residual plot will show no fitted
discernible pattern. The presence of a pattern may indicate a problem with
some aspect of the linear model.

>If the residual plot indicates that there are non-linear associations in the
data, then a simple approach is to use non-linear transformations of the
predictors, such as log X,
√X, and X2, in the regression model.

>if the error terms are correlated, we may have an
unwarranted sense of confidence in our model.

>If in fact there
is correlation among the error terms, then the estimated standard errors
will tend to underestimate the true standard errors.


>As an extreme example, suppose we accidentally doubled our data, leading
to observations and error terms identical in pairs. If we ignored this, our
standard error calculations would be as if we had a sample of size 2n, when
in fact we have only n samples. Our estimated parameters would be the
same for the 2n samples as for the n samples, but the confidence intervals
would be narrower by a factor of √
2!

> In general, the
assumption of uncorrelated errors is extremely important for linear regression
as well as for other statistical methods, and good experimental design
is crucial in order to mitigate the risk of such correlations.


>it is often the case that the variances of the error terms are
non-constant. For instance, the variances of the error terms may increase
with the value of the response. One can identify non-constant variances in
the errors, or heteroscedasticity, from the presence of a funnel shape in heterosceda- the residual plot

> When faced with this problem, one possible solution is to transform
the response Y using a concave function such as log Y or √
Y . Such
a transformation results in a greater amount of shrinkage of the larger responses,
leading to a reduction in heteroscedasticity


>Sometimes we have a good idea of the variance of each response. For
example, the ith response could be an average of ni raw observations. If
each of these raw observations is uncorrelated with variance σ2, then their
average has variance σ2
i = σ2/ni. In this case a simple remedy is to fit our
model by weighted least squares, with weights proportional to the inverse weighted
variances—i.e. wi = ni in this case. Most linear regression software allows least squares
for observation weights

>Residual plots can be used to identify outliers.

> in practice, it can be difficult to decide how large a residual
needs to be before we consider the point to be an outlier. To address
this problem, instead of plotting the residuals, we can plot the studentized
residuals, computed by dividing each residual ei by its estimated standard studentized
error. Observations whose studentized residuals are greater than 3 in abso- residual
lute value are possible outliers


>observations with high leverage high leverage
have an unusual value for xi.

>leverage
statistic

>the power of the hypothesis test—the probability of correctly power
detecting a non-zero coefficient—is reduced by collinearity

>A simple way to detect collinearity is to look at the correlation matrix
of the predictors. An element of this matrix that is large in absolute value
indicates a pair of highly correlated variables, and therefore a collinearity
problem in the data. Unfortunately, not all collinearity problems can be
detected by inspection of the correlation matrix: it is possible for collinearity
to exist between three or more variables even if no pair of variables
has a particularly high correlation. We call this situation multicollinearity

>Instead of inspecting the correlation matrix, a better way to assess multi- collinearity
collinearity is to compute the variance inflation factor (VIF)

>there are two simple solutions.
The first is to drop one of the problematic variables from the regression.
This can usually be done without much compromise to the regression
fit, since the presence of collinearity implies that the information that this
variable provides about the response is redundant in the presence of the
other variables

>The second solution is
to combine the collinear variables together into a single predictor


>linear regression is an example of a parametric
approach because it assumes a linear functional form for f(X). Parametric
methods have several advantages. They are often easy to fit, because one
need estimate only a small number of coefficients. In the case of linear regression,
the coefficients have simple interpretations, and tests of statistical
significance can be easily performed. But parametric methods do have a
disadvantage: by construction, they make strong assumptions about the
form of f(X). If the specified functional form is far from the truth, and
prediction accuracy is our goal, then the parametric method will perform
poorly


>, non-parametric methods do not explicitly assume a parametric
form for f(X), and thereby provide an alternative and more flexible
approach for performing regression
K-nearest neighbors regression (KNN regression)

>the optimal value
for K will depend on the bias-variance tradeof

>the parametric approach will outperform the nonparametric
approach if the parametric form that has been selected is close
to the true form of f

>As a general rule,
parametric methods will tend to outperform non-parametric approaches
when there is a small number of observations per predictor.

>library(), fix(), summary(), names(), nrow(), dim(), attach()

>lm(y~x, data), summary(lm(y~x, data)),
coef(), confint(),predict(model,df)

>plot(x,y), abline(model),plot.new(),
plot(lm(model))

>par(mfrow=c(2,2)), plot(model),
residuals(model), rstudent(model)

>hatvalues(model), which.max(hatvalues(model))

>lm.fit(y~.,data), summary(model)$r.sq,
summary(lm.fit)$sigma

>librar("car")

>sessionInfo(), CRAN_package_db(), version, vif(model)

>coef(), cofint(), predict(), update(model, y~.-age)

>summary (lm(medv~(lstat:age) ,data=Boston)),
summary (lm(medv~(lstat + age + lstat:age) ,data=Boston)),
summary (lm(medv~(lstat*age) ,data=Boston))

>lm.fit2=lm(medv~lstat+I(lstat^2)), lm.fit5 = lm(medv~poly(lstat, 5))

>Carseats = ISLR::Carseats, packageVersion("ISLR"),
package_version(R.version),
getRversion(),lm.fit=lm(Sales~.+Income :Advertising +Price:Age ,data=Carseats ),
contrasts (ShelveLoc)
c

>P = 1/1+ e^-(a+bx)

>exp(1.563)=4.773119

Probability = 4.773119 / (1 + 4.773119) = 0.8267
